{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5110)\n",
      "/opt/conda/envs/altair/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu'\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/cuda/bin/\"\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP, \\\n",
    "                                Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, LSTM\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "# from copy import copy\n",
    "import numpy as np\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "import learningfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from redbaron import RedBaron\n",
    "\n",
    "with open(\"sample.py\", \"r\") as f:\n",
    "    source = f.read()\n",
    "with open(\"sample.py\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "red = RedBaron(source)\n",
    "data = []\n",
    "for fn_node in red.findAll(\"DefNode\"):\n",
    "    starting_line = fn_node.absolute_bounding_box.top_left.to_tuple()[0]\n",
    "    ending_line = fn_node.absolute_bounding_box.bottom_right.to_tuple()[0]\n",
    "    fn_lines = lines[starting_line-1:ending_line-1]\n",
    "    data.append(\"\".join(fn_lines).rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ['this is data', 'we like data', 'once upon a time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('test.txt', 'r') as f:\n",
    "    char_str = f.readlines()[0]\n",
    "char_to_idx = {char:idx for (idx, char) in enumerate(char_str)}\n",
    "idx_to_char = {idx:char for (idx, char) in enumerate(char_str)}\n",
    "\n",
    "SOS_TOKEN_IDX = len(char_to_idx)\n",
    "EOS_TOKEN_IDX = len(char_to_idx)+1\n",
    "\n",
    "# if desired_length is not specified, desired_length will be len(document)\n",
    "# if len(document) < desired_length, pad with zero vectors to reach desired_length\n",
    "# if len(document) > desired_length, truncate at desired_length\n",
    "# encoded will always be desired_length + 2 (to include SOS and EOS tokens)\n",
    "# EOS token will always be appended to the end of encoded even if len(document) > desired_length\n",
    "def encode_document(document, char_to_idx, desired_length=-1):\n",
    "    if desired_length == -1:\n",
    "        desired_length = len(document)\n",
    "    encoded = np.zeros((desired_length+2, len(char_to_idx)+2)) # +2 for SOS and EOS tokens\n",
    "    encoded[0, SOS_TOKEN_IDX] = 1 # set SOS token\n",
    "    for doc_idx, char in enumerate(document[:desired_length]):\n",
    "        encoded[doc_idx+1, char_to_idx[char]] = 1\n",
    "    encoded[len(document[:desired_length])+1, EOS_TOKEN_IDX] = 1\n",
    "    return encoded.reshape(encoded.shape[0], 1, encoded.shape[1])\n",
    "\n",
    "# if desired_length is not specified, desired_length will be length of longest document\n",
    "def encode_documents(documents, char_to_idx, desired_length=-1):\n",
    "    if desired_length == -1:\n",
    "        desired_length = max([len(document) for document in documents])\n",
    "    encodeds = []\n",
    "    for document in documents:\n",
    "        encodeds.append(encode_document(document, char_to_idx, desired_length))\n",
    "    e = np.array(encodeds)\n",
    "    return e\n",
    "\n",
    "# encoded must be one-hot\n",
    "def decode_document(encoded, idx_to_char):\n",
    "    decoded = \"\"\n",
    "    for idx in np.nonzero(encoded)[2]:\n",
    "        if idx == SOS_TOKEN_IDX:\n",
    "            continue\n",
    "        elif idx == EOS_TOKEN_IDX:\n",
    "            break\n",
    "        decoded += idx_to_char[idx]\n",
    "    return decoded\n",
    "\n",
    "def decode_documents(encodeds, idx_to_char):\n",
    "    decodeds = []\n",
    "    for encoded in encodeds:\n",
    "        decodeds.append(decode_document(encoded, idx_to_char))\n",
    "    return decodeds\n",
    "\n",
    "#encoded = encode_document(\"this is a\", char_to_idx)\n",
    "#decode_document(encoded, idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded = encode_documents(data, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_conversion(predictions):\n",
    "    converted = np.zeros(predictions.shape)\n",
    "    for prediction_idx, prediction in enumerate(predictions):\n",
    "        for elem_idx, elem in enumerate(prediction):\n",
    "            converted[prediction_idx, elem_idx, 0, np.argmax(elem[0])] = 1\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.tensor4('X')\n",
    "rnnType = 'gru'\n",
    "dimIn = 98\n",
    "dim = 20\n",
    "wtstd = 0.2\n",
    "rnnbias_init = Constant(0.0)\n",
    "rnnwt_init = IsotropicGaussian(wtstd)\n",
    "linewt_init = IsotropicGaussian(wtstd)\n",
    "line_bias = Constant(1.0)\n",
    "\n",
    "lr = 0.0001\n",
    "decay = 0.9\n",
    "decay_itr = 15000\n",
    "learning_rate = theano.shared(np.array(lr, dtype=theano.config.floatX))\n",
    "learning_decay = np.array(decay, dtype=theano.config.floatX)\n",
    "\n",
    "clippings = 0.3\n",
    "#ADD lr decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def onestepEnc(X):\n",
    "    data1, data2 = fork.apply(X) \n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hEnc = rnn.apply(data1, data2) \n",
    "    else:\n",
    "        hEnc, _ = rnn.apply(data2)\n",
    "\n",
    "    return hEnc\n",
    "\n",
    "hEnc, _ = theano.scan(onestepEnc, X) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fork.initialize()\n",
    "rnn.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer1Fun = theano.function([X], hEnc, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeData = np.random.rand(3,50,1,70).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if rnnType == 'gru':\n",
    "    rnn2 = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn2 = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "fork2 = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "\n",
    "\n",
    "def onestepEnc2(hEnc):\n",
    "    data3, data4 = fork2.apply(hEnc) \n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hEnc2 = rnn2.apply(data3, data4) \n",
    "    else:\n",
    "        hEnc2, _ = rnn2.apply(data4)\n",
    "\n",
    "    return hEnc2, data3\n",
    "\n",
    "[hEnc2, data3], _ = theano.scan(onestepEnc2, hEnc) \n",
    "\n",
    "fork2.initialize()\n",
    "rnn2.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer2Fun = theano.function([X], hEnc2, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if rnnType == 'gru':\n",
    "    rnn3 = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn3 = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "fork3 = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "forkD = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dimIn, dimIn * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "def onestepEnc3(hEnc2):\n",
    "    data5, data6 = fork3.apply(hEnc2) \n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hEnc3 = rnn3.apply(data5, data6) \n",
    "    else:\n",
    "        hEnc3, _ = rnn3.apply(data6)\n",
    "\n",
    "    return hEnc3\n",
    "\n",
    "hEnc3, _ = theano.scan(onestepEnc3, hEnc2) \n",
    "h4decoder = hEnc3[:,-1,:,:].reshape((-1, 1,1,20))\n",
    "\n",
    "h4reshape, _ = forkD.apply(h4decoder)\n",
    "\n",
    "forkD.initialize()\n",
    "fork3.initialize()\n",
    "rnn3.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer3Fun = theano.function([X], h4reshape, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if rnnType == 'gru':\n",
    "    rnn4 = GatedRecurrent(dim=dimIn, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn4 = LSTM(dim=dimIn, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "#TOTHINK: transform before the decoder or after\n",
    "fork4 = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIn, output_dims=[dimIn, dimIn * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "targets = T.concatenate((h4reshape, X[:,:-1, :,:]), axis=1)\n",
    "\n",
    "def onestepEnc4(targets):\n",
    "    data7, data8 = fork4.apply(targets) \n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hDec = rnn4.apply(data7, data8) \n",
    "    else:\n",
    "        hDec, _ = rnn4.apply(data8)\n",
    "\n",
    "    return hDec\n",
    "\n",
    "hDec, _ = theano.scan(onestepEnc4, targets) \n",
    "\n",
    "fork4.initialize()\n",
    "rnn4.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer4Fun = theano.function([X], hDec, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decFun = theano.function([X], targets, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predTargets = T.exp(hDec)/T.sum(T.exp(hDec), axis=(3,2), keepdims=True)\n",
    "#precost = -X.squeeze*T.log(predTargets.squeeze()) - (1-X.squeeze())*T.log(1-predTargets.squeeze())\n",
    "\n",
    "#ADDLATER: beam search\n",
    "cost = T.mean(T.sum(T.nnet.categorical_crossentropy(predTargets, X), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling graph you talented soul\n"
     ]
    }
   ],
   "source": [
    "cg = ComputationGraph([cost])\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "\n",
    "###To check gradients for explosion/shrinkage\n",
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, clippings)\n",
    "gradientFun = theano.function([X, predTargets], gradients, allow_input_downcast=True)\n",
    "\n",
    "learning = learningfunctions.Learning(cost,params,learning_rate,l1=0.,l2=0.,maxnorm=0.,c=clippings)\n",
    "updates = learning.Adam() \n",
    "\n",
    "print('compiling graph you talented soul')\n",
    "classifierTrain = theano.function([X], [cost, predTargets], \n",
    "                                  updates=updates, allow_input_downcast=True)\n",
    "#classifierPredict = theano.function([X], [softoutClass, attEncpred, attContextpred], allow_input_downcast=True)\n",
    "classifierPredict = theano.function([X], predTargets, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forks = [fork, fork2, fork3, fork4, forkD]\n",
    "rnns = [rnn, rnn2, rnn3, rnn4]\n",
    "\n",
    "for fork in forks:\n",
    "    fork.initialize()\n",
    "for rnn in rnns:\n",
    "    rnn.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.50408172607422: ['~ld~~jj~jjjjjj~jjj', '~l~~~j~j~jjjjj~jjj', '~~~~~~jj~~~~jjjj~]']\n",
      "  gradient norms:  0.229775\n",
      "  gradient norms:  0.0831331\n",
      "  gradient norms:  0.0190914\n",
      "  gradient norms:  0.0127768\n",
      "  gradient norms:  0.0221939\n",
      "  gradient norms:  0.0515382\n",
      "  gradient norms:  0.137612\n",
      "  gradient norms:  0.0211662\n",
      "  gradient norms:  0.0657096\n",
      "  gradient norms:  0.0267837\n",
      "  gradient norms:  0.00579115\n",
      "  gradient norms:  0.000284334\n",
      "  gradient norms:  0.00191638\n",
      "  gradient norms:  0.0061682\n",
      "  gradient norms:  0.0123824\n",
      "  gradient norms:  0.0401833\n",
      "  gradient norms:  0.00965748\n",
      "  gradient norms:  0.00205743\n",
      "  gradient norms:  5.39623e-05\n",
      "  gradient norms:  0.000635043\n",
      "  gradient norms:  0.00217\n",
      "  gradient norms:  0.0041517\n",
      "  gradient norms:  0.0142125\n",
      "  gradient norms:  0.00499458\n",
      "  gradient norms:  0.00101761\n",
      "  gradient norms:  3.07807e-05\n",
      "  gradient norms:  0.000297811\n",
      "  gradient norms:  0.000115647\n",
      "  gradient norms:  0.00201731\n",
      "  gradient norms:  0.000639412\n",
      "43.878875732421875: ['wheee            ', 'hweee  e          ', 'hwneee            ']\n",
      "  gradient norms:  0.201718\n",
      "  gradient norms:  0.19893\n",
      "  gradient norms:  0.0265727\n",
      "  gradient norms:  0.0114114\n",
      "  gradient norms:  0.05163\n",
      "  gradient norms:  0.0162202\n",
      "  gradient norms:  0.0768266\n",
      "  gradient norms:  0.0014788\n",
      "  gradient norms:  0.00667589\n",
      "  gradient norms:  0.00191826\n",
      "  gradient norms:  0.000451131\n",
      "  gradient norms:  2.95714e-06\n",
      "  gradient norms:  0.000105459\n",
      "  gradient norms:  0.000372816\n",
      "  gradient norms:  0.000640421\n",
      "  gradient norms:  0.00194531\n",
      "  gradient norms:  0.00172667\n",
      "  gradient norms:  0.000254652\n",
      "  gradient norms:  1.4529e-06\n",
      "  gradient norms:  8.28047e-05\n",
      "  gradient norms:  0.000246653\n",
      "  gradient norms:  0.000822634\n",
      "  gradient norms:  0.00237103\n",
      "  gradient norms:  0.0019247\n",
      "  gradient norms:  0.000387632\n",
      "  gradient norms:  3.28936e-05\n",
      "  gradient norms:  0.000134221\n",
      "  gradient norms:  8.41348e-05\n",
      "  gradient norms:  0.000826237\n",
      "  gradient norms:  0.000738832\n",
      "42.33331298828125: ['whis             ', 'we               ', 'once             ']\n",
      "  gradient norms:  0.214043\n",
      "  gradient norms:  0.155216\n",
      "  gradient norms:  0.0349777\n",
      "  gradient norms:  0.0142847\n",
      "  gradient norms:  0.0369786\n",
      "  gradient norms:  0.0208644\n",
      "  gradient norms:  0.065183\n",
      "  gradient norms:  0.00118124\n",
      "  gradient norms:  0.00546885\n",
      "  gradient norms:  0.00166502\n",
      "  gradient norms:  0.000382147\n",
      "  gradient norms:  2.78964e-06\n",
      "  gradient norms:  8.74327e-05\n",
      "  gradient norms:  0.000425743\n",
      "  gradient norms:  0.000561373\n",
      "  gradient norms:  0.0020074\n",
      "  gradient norms:  0.000960078\n",
      "  gradient norms:  0.000242312\n",
      "  gradient norms:  4.55106e-06\n",
      "  gradient norms:  7.98541e-05\n",
      "  gradient norms:  0.000259168\n",
      "  gradient norms:  0.000253177\n",
      "  gradient norms:  0.000919361\n",
      "  gradient norms:  0.000866646\n",
      "  gradient norms:  0.000151693\n",
      "  gradient norms:  2.38122e-05\n",
      "  gradient norms:  4.77101e-05\n",
      "  gradient norms:  4.00914e-05\n",
      "  gradient norms:  0.000292103\n",
      "  gradient norms:  0.000285388\n",
      "41.851409912109375: ['whis             ', 'we               ', 'wnce             ']\n",
      "  gradient norms:  0.0955545\n",
      "  gradient norms:  0.0573043\n",
      "  gradient norms:  0.0264802\n",
      "  gradient norms:  0.00240645\n",
      "  gradient norms:  0.0256643\n",
      "  gradient norms:  0.00276692\n",
      "  gradient norms:  0.0355403\n",
      "  gradient norms:  0.000207869\n",
      "  gradient norms:  0.000866009\n",
      "  gradient norms:  0.000202998\n",
      "  gradient norms:  6.23865e-05\n",
      "  gradient norms:  3.23046e-07\n",
      "  gradient norms:  1.55465e-05\n",
      "  gradient norms:  6.15979e-05\n",
      "  gradient norms:  6.93581e-05\n",
      "  gradient norms:  0.000261728\n",
      "  gradient norms:  0.00015386\n",
      "  gradient norms:  4.59771e-05\n",
      "  gradient norms:  1.98475e-06\n",
      "  gradient norms:  1.36912e-05\n",
      "  gradient norms:  4.6496e-05\n",
      "  gradient norms:  5.52538e-05\n",
      "  gradient norms:  0.000187431\n",
      "  gradient norms:  0.000224198\n",
      "  gradient norms:  4.2014e-05\n",
      "  gradient norms:  2.79388e-06\n",
      "  gradient norms:  1.45633e-05\n",
      "  gradient norms:  9.58203e-06\n",
      "  gradient norms:  9.50891e-05\n",
      "  gradient norms:  7.32871e-05\n",
      "41.67299270629883: ['whis             ', 'we               ', 'once             ']\n",
      "  gradient norms:  0.102369\n",
      "  gradient norms:  0.0918977\n",
      "  gradient norms:  0.0252998\n",
      "  gradient norms:  0.00896634\n",
      "  gradient norms:  0.0183021\n",
      "  gradient norms:  0.0134138\n",
      "  gradient norms:  0.0205879\n",
      "  gradient norms:  0.000512211\n",
      "  gradient norms:  0.00214699\n",
      "  gradient norms:  0.000878596\n",
      "  gradient norms:  0.000201327\n",
      "  gradient norms:  1.01938e-06\n",
      "  gradient norms:  5.17034e-05\n",
      "  gradient norms:  0.000153557\n",
      "  gradient norms:  0.000302409\n",
      "  gradient norms:  0.000887007\n",
      "  gradient norms:  0.000721728\n",
      "  gradient norms:  0.000121742\n",
      "  gradient norms:  5.14637e-06\n",
      "  gradient norms:  3.99444e-05\n",
      "  gradient norms:  0.000119986\n",
      "  gradient norms:  0.000267423\n",
      "  gradient norms:  0.0007921\n",
      "  gradient norms:  0.00129143\n",
      "  gradient norms:  0.00024786\n",
      "  gradient norms:  4.3537e-05\n",
      "  gradient norms:  8.29197e-05\n",
      "  gradient norms:  5.84558e-05\n",
      "  gradient norms:  0.00044671\n",
      "  gradient norms:  0.00039086\n",
      "41.56550598144531: ['whis             ', 'we               ', 'once             ']\n",
      "  gradient norms:  0.0996619\n",
      "  gradient norms:  0.0994667\n",
      "  gradient norms:  0.0237162\n",
      "  gradient norms:  0.00991826\n",
      "  gradient norms:  0.0182449\n",
      "  gradient norms:  0.0138947\n",
      "  gradient norms:  0.019403\n",
      "  gradient norms:  0.000575041\n",
      "  gradient norms:  0.00239649\n",
      "  gradient norms:  0.00102578\n",
      "  gradient norms:  0.000290279\n",
      "  gradient norms:  1.41016e-06\n",
      "  gradient norms:  7.50964e-05\n",
      "  gradient norms:  0.00021461\n",
      "  gradient norms:  0.00035035\n",
      "  gradient norms:  0.00100739\n",
      "  gradient norms:  0.00109041\n",
      "  gradient norms:  0.000191029\n",
      "  gradient norms:  7.07321e-06\n",
      "  gradient norms:  6.45954e-05\n",
      "  gradient norms:  0.000192737\n",
      "  gradient norms:  0.000385676\n",
      "  gradient norms:  0.00114961\n",
      "  gradient norms:  0.00192078\n",
      "  gradient norms:  0.000370881\n",
      "  gradient norms:  6.79724e-05\n",
      "  gradient norms:  0.000121293\n",
      "  gradient norms:  9.60392e-05\n",
      "  gradient norms:  0.00060612\n",
      "  gradient norms:  0.000646467\n",
      "41.48820495605469: ['whis             ', 'we               ', 'once             ']\n",
      "  gradient norms:  0.0230394\n",
      "  gradient norms:  0.0276929\n",
      "  gradient norms:  0.0161922\n",
      "  gradient norms:  0.00011831\n",
      "  gradient norms:  0.013031\n",
      "  gradient norms:  0.000681199\n",
      "  gradient norms:  0.00689985\n",
      "  gradient norms:  1.36247e-05\n",
      "  gradient norms:  5.27074e-05\n",
      "  gradient norms:  8.86354e-06\n",
      "  gradient norms:  1.98336e-06\n",
      "  gradient norms:  3.49019e-08\n",
      "  gradient norms:  3.41628e-07\n",
      "  gradient norms:  3.03281e-06\n",
      "  gradient norms:  2.95816e-06\n",
      "  gradient norms:  1.00157e-05\n",
      "  gradient norms:  5.44502e-06\n",
      "  gradient norms:  1.65476e-06\n",
      "  gradient norms:  1.60092e-07\n",
      "  gradient norms:  5.00337e-07\n",
      "  gradient norms:  1.67509e-06\n",
      "  gradient norms:  2.09534e-06\n",
      "  gradient norms:  6.64294e-06\n",
      "  gradient norms:  5.74807e-06\n",
      "  gradient norms:  1.24465e-06\n",
      "  gradient norms:  4.86453e-07\n",
      "  gradient norms:  4.6712e-07\n",
      "  gradient norms:  3.24117e-07\n",
      "  gradient norms:  2.57236e-06\n",
      "  gradient norms:  2.17291e-06\n",
      "41.42385482788086: ['this             ', 'te li            ', 'once             ']\n",
      "  gradient norms:  0.0186511\n",
      "  gradient norms:  0.0272308\n",
      "  gradient norms:  0.00991754\n",
      "  gradient norms:  0.000135101\n",
      "  gradient norms:  0.0123065\n",
      "  gradient norms:  0.000521897\n",
      "  gradient norms:  0.00470044\n",
      "  gradient norms:  5.39178e-05\n",
      "  gradient norms:  0.000200588\n",
      "  gradient norms:  6.80732e-05\n",
      "  gradient norms:  1.35296e-05\n",
      "  gradient norms:  2.3524e-07\n",
      "  gradient norms:  3.47461e-06\n",
      "  gradient norms:  1.25754e-05\n",
      "  gradient norms:  2.44023e-05\n",
      "  gradient norms:  7.31021e-05\n",
      "  gradient norms:  6.051e-05\n",
      "  gradient norms:  1.11203e-05\n",
      "  gradient norms:  9.48318e-07\n",
      "  gradient norms:  3.42135e-06\n",
      "  gradient norms:  1.06973e-05\n",
      "  gradient norms:  1.70197e-05\n",
      "  gradient norms:  5.62447e-05\n",
      "  gradient norms:  9.84867e-05\n",
      "  gradient norms:  1.91496e-05\n",
      "  gradient norms:  2.52389e-06\n",
      "  gradient norms:  6.16466e-06\n",
      "  gradient norms:  4.20052e-06\n",
      "  gradient norms:  3.0644e-05\n",
      "  gradient norms:  2.95602e-05\n",
      "41.36798095703125: ['this             ', 'te li            ', 'once             ']\n",
      "  gradient norms:  0.0124879\n",
      "  gradient norms:  0.0244205\n",
      "  gradient norms:  0.00942025\n",
      "  gradient norms:  5.80863e-05\n",
      "  gradient norms:  0.0109145\n",
      "  gradient norms:  0.000345872\n",
      "  gradient norms:  0.00303629\n",
      "  gradient norms:  6.47567e-06\n",
      "  gradient norms:  3.2258e-05\n",
      "  gradient norms:  7.4236e-06\n",
      "  gradient norms:  2.2171e-06\n",
      "  gradient norms:  2.97265e-08\n",
      "  gradient norms:  5.73811e-07\n",
      "  gradient norms:  1.79286e-06\n",
      "  gradient norms:  2.58161e-06\n",
      "  gradient norms:  8.25999e-06\n",
      "  gradient norms:  5.84095e-06\n",
      "  gradient norms:  1.22462e-06\n",
      "  gradient norms:  1.22723e-07\n",
      "  gradient norms:  3.74854e-07\n",
      "  gradient norms:  1.15496e-06\n",
      "  gradient norms:  2.52581e-06\n",
      "  gradient norms:  7.46147e-06\n",
      "  gradient norms:  1.06578e-05\n",
      "  gradient norms:  1.64585e-06\n",
      "  gradient norms:  4.51783e-07\n",
      "  gradient norms:  5.35722e-07\n",
      "  gradient norms:  5.99817e-07\n",
      "  gradient norms:  4.35667e-06\n",
      "  gradient norms:  4.44089e-06\n",
      "41.335567474365234: ['this             ', 'te li            ', 'once             ']\n",
      "  gradient norms:  0.043048\n",
      "  gradient norms:  0.0782531\n",
      "  gradient norms:  0.00911129\n",
      "  gradient norms:  0.00765868\n",
      "  gradient norms:  0.0116115\n",
      "  gradient norms:  0.00665837\n",
      "  gradient norms:  0.0174823\n",
      "  gradient norms:  0.0024253\n",
      "  gradient norms:  0.00969556\n",
      "  gradient norms:  0.00372665\n",
      "  gradient norms:  0.000844504\n",
      "  gradient norms:  2.49713e-06\n",
      "  gradient norms:  0.000220786\n",
      "  gradient norms:  0.000768935\n",
      "  gradient norms:  0.00132462\n",
      "  gradient norms:  0.00434717\n",
      "  gradient norms:  0.0040757\n",
      "  gradient norms:  0.00085857\n",
      "  gradient norms:  2.65161e-05\n",
      "  gradient norms:  0.000252385\n",
      "  gradient norms:  0.000802448\n",
      "  gradient norms:  0.00134745\n",
      "  gradient norms:  0.00426485\n",
      "  gradient norms:  0.00848558\n",
      "  gradient norms:  0.00153872\n",
      "  gradient norms:  3.35199e-05\n",
      "  gradient norms:  0.000526223\n",
      "  gradient norms:  0.000290686\n",
      "  gradient norms:  0.00365506\n",
      "  gradient norms:  0.00212482\n",
      "41.31146240234375: ['this     a       ', 'te  i    a       ', 'once   o         ']\n",
      "  gradient norms:  0.0117699\n",
      "  gradient norms:  0.0267441\n",
      "  gradient norms:  0.00823059\n",
      "  gradient norms:  0.00197594\n",
      "  gradient norms:  0.00862507\n",
      "  gradient norms:  0.00154866\n",
      "  gradient norms:  0.00479195\n",
      "  gradient norms:  0.000644007\n",
      "  gradient norms:  0.00257902\n",
      "  gradient norms:  0.00102162\n",
      "  gradient norms:  0.000233359\n",
      "  gradient norms:  6.7183e-07\n",
      "  gradient norms:  6.08764e-05\n",
      "  gradient norms:  0.000210687\n",
      "  gradient norms:  0.000359326\n",
      "  gradient norms:  0.00118368\n",
      "  gradient norms:  0.00113943\n",
      "  gradient norms:  0.000236689\n",
      "  gradient norms:  7.21495e-06\n",
      "  gradient norms:  6.94125e-05\n",
      "  gradient norms:  0.000219818\n",
      "  gradient norms:  0.000375732\n",
      "  gradient norms:  0.00118467\n",
      "  gradient norms:  0.00240152\n",
      "  gradient norms:  0.000437166\n",
      "  gradient norms:  9.1009e-06\n",
      "  gradient norms:  0.000149525\n",
      "  gradient norms:  8.18284e-05\n",
      "  gradient norms:  0.0010349\n",
      "  gradient norms:  0.000596814\n",
      "41.2928581237793: ['this     a     aa', 'te  i    a     aa', 'once   o         ']\n",
      "  gradient norms:  0.00479756\n",
      "  gradient norms:  0.01703\n",
      "  gradient norms:  0.00760669\n",
      "  gradient norms:  3.18954e-05\n",
      "  gradient norms:  0.00761548\n",
      "  gradient norms:  9.67331e-05\n",
      "  gradient norms:  0.000681643\n",
      "  gradient norms:  5.80532e-06\n",
      "  gradient norms:  2.35834e-05\n",
      "  gradient norms:  7.83948e-06\n",
      "  gradient norms:  1.8349e-06\n",
      "  gradient norms:  4.4266e-09\n",
      "  gradient norms:  4.80144e-07\n",
      "  gradient norms:  1.66962e-06\n",
      "  gradient norms:  2.72333e-06\n",
      "  gradient norms:  9.33647e-06\n",
      "  gradient norms:  9.57702e-06\n",
      "  gradient norms:  1.88303e-06\n",
      "  gradient norms:  1.72606e-08\n",
      "  gradient norms:  5.57752e-07\n",
      "  gradient norms:  1.73773e-06\n",
      "  gradient norms:  3.11901e-06\n",
      "  gradient norms:  9.677e-06\n",
      "  gradient norms:  2.04211e-05\n",
      "  gradient norms:  3.73799e-06\n",
      "  gradient norms:  6.33163e-08\n",
      "  gradient norms:  1.27884e-06\n",
      "  gradient norms:  6.89912e-07\n",
      "  gradient norms:  8.7935e-06\n",
      "  gradient norms:  5.0102e-06\n",
      "41.25392150878906: ['this     a   ', 'te  i    a   ', 'once   o         ']\n",
      "  gradient norms:  0.00371057\n",
      "  gradient norms:  0.0253053\n",
      "  gradient norms:  0.105565\n",
      "  gradient norms:  0.002116\n",
      "  gradient norms:  0.0067593\n",
      "  gradient norms:  6.78032e-05\n",
      "  gradient norms:  0.00045893\n",
      "  gradient norms:  8.69293e-06\n",
      "  gradient norms:  3.5114e-05\n",
      "  gradient norms:  1.17855e-05\n",
      "  gradient norms:  2.74234e-06\n",
      "  gradient norms:  7.34984e-09\n",
      "  gradient norms:  7.14973e-07\n",
      "  gradient norms:  2.45153e-06\n",
      "  gradient norms:  4.07925e-06\n",
      "  gradient norms:  1.36505e-05\n",
      "  gradient norms:  1.36601e-05\n",
      "  gradient norms:  2.75832e-06\n",
      "  gradient norms:  6.08432e-08\n",
      "  gradient norms:  8.16806e-07\n",
      "  gradient norms:  2.55976e-06\n",
      "  gradient norms:  4.50702e-06\n",
      "  gradient norms:  1.40297e-05\n",
      "  gradient norms:  2.92954e-05\n",
      "  gradient norms:  5.38625e-06\n",
      "  gradient norms:  8.9295e-08\n",
      "  gradient norms:  1.84322e-06\n",
      "  gradient norms:  9.92383e-07\n",
      "  gradient norms:  1.26414e-05\n",
      "  gradient norms:  7.19529e-06\n",
      "41.22418212890625: ['this     a   ', 'te  i    a   ', 'once   o         ']\n",
      "  gradient norms:  0.00300606\n",
      "  gradient norms:  0.0131187\n",
      "  gradient norms:  0.0973355\n",
      "  gradient norms:  0.0001712\n",
      "  gradient norms:  0.00594279\n",
      "  gradient norms:  0.000114295\n",
      "  gradient norms:  0.000370693\n",
      "  gradient norms:  2.72017e-05\n",
      "  gradient norms:  0.000108917\n",
      "  gradient norms:  4.42376e-05\n",
      "  gradient norms:  1.02896e-05\n",
      "  gradient norms:  3.28765e-08\n",
      "  gradient norms:  2.66289e-06\n",
      "  gradient norms:  9.13591e-06\n",
      "  gradient norms:  1.53037e-05\n",
      "  gradient norms:  4.98959e-05\n",
      "  gradient norms:  4.72659e-05\n",
      "  gradient norms:  1.00382e-05\n",
      "  gradient norms:  4.42924e-07\n",
      "  gradient norms:  2.97115e-06\n",
      "  gradient norms:  9.42642e-06\n",
      "  gradient norms:  1.56137e-05\n",
      "  gradient norms:  4.93077e-05\n",
      "  gradient norms:  9.97489e-05\n",
      "  gradient norms:  1.83921e-05\n",
      "  gradient norms:  5.64807e-07\n",
      "  gradient norms:  6.2719e-06\n",
      "  gradient norms:  3.43218e-06\n",
      "  gradient norms:  4.27308e-05\n",
      "  gradient norms:  2.49118e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-97013b5cca41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_predTargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifierTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mresult_converted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_predTargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/altair/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs=100000\n",
    "for itr, x in enumerate(range(num_epochs)):\n",
    "    result_cost, result_predTargets = classifierTrain(encoded)\n",
    "    result_converted = one_hot_conversion(result_predTargets)\n",
    "    if itr % 1000 == 0:\n",
    "        print(\"%s: %s\" % (result_cost, decode_documents(result_converted, idx_to_char)))\n",
    "        grads = gradientFun(encoded, result_predTargets)\n",
    "        for gra in grads:\n",
    "            print('  gradient norms: ', np.linalg.norm(gra))\n",
    "    if itr % decay_itr == 0:\n",
    "        learning_rate.set_value(learning_rate.get_value() * learning_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = classifierPredict(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "converted = one_hot_conversion(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode_documents(converted, idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "altair",
   "language": "python",
   "name": "altair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
